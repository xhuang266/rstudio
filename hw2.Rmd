---
title: "Homework 2"
author: "Elaine Huang"
date: "2/22/2018"
output: html_document
---

#13.7.1

The null hyphtheses are TV, radio and newspaper have no effect on advertising products. Based on the Table, p-values for TV and radio are extremely small, thus we can reject the null hyphtheses and retain H1. P-value for new paper cannot be used to support rejecting the null hypothese. We can conclude TV and radio both affect product sales, while newspaper has no effect. 


#13.7.3

(a)
According to the questin, the least square line is y = 50 + 20*GPA + 0.07*IQ + 35*gender + 0.01*GPA×IQ - 10*GPA×Gender

  Since we define 1 for Female and 0 for Male

  Female   y = 85 + 10*GPA + 0.07*IQ + 0.01*GPA×IQ
  
  Male     y = 50 + 20*GPA + 0.07*IQ  + 0.01*GPA×IQ

  When IQ and GPA are fixed, difference = (85+10*GPA)-(50+20*GPA) = 35-10*GPA, which only   depends on GPA. With a GPA less than 3.5, female earns more. With a GPA higher than 3.5, male earns more. When GPA is 3.5, it is equal. Thus, iii is the correct answer.

(b)
y = 85 + 10*GPA + 0.07*IQ + 0.01*GPA×IQ = 85 + 10*4.0 + 0.07*110 + 0.01*440 = 137.1

(c)
False. Though the coeffient might be small, it does not mean interaection of GPA and IQ does not have an impact on the quality of the model. In order to determine whether it has a relationship with the model, we have to test the null hypothese β1 = 0. Then, we can conclude if there is a relationship based on p value and F-statistic. If F-statistic is greater than 1, we reject H0 and conclude there exists relationship. If F-statistic is close to 1, we cannot reject H0 and conclude there is no relationship.


#13.7.4

(a)
Since th true relationship is linear, we can predict linear regression may fit the data closer than cubic regression. Thus, linear regression has a lower RSS as well.

(b)
Polynomial regression normally fits unnecessarily wiggly, so we expect this overfitting the data and having higher RSS. Consequently, polynomial regression causes more error.

(c)
Polynomail regression has a better fit due to its highly flexibility when the true relationship is not linear.  Consequently, polynomial regression has lower training RSS and less error.

(d)
It is not clear which one fits closer, because we do not know whether the real fit is more closer to linear fit or more closer to cubic fit. If it is more closer to linear fit, linear regression will have lower training RSS and less error. If it is more closer to cubic fit, cubic regression will have lower training RSS and less error.


#13.7.15

Answer here (with R Code if applicable).

```{r}
library(MASS)
attach(Boston)
fit.zn <- lm(crim ~ zn)
summary(fit.zn)
fit.indus <- lm(crim ~ indus)
summary(fit.indus)
chas <- as.factor(chas)
fit.chas <- lm(crim ~ chas)
summary(fit.chas)
fit.nox <- lm(crim ~ nox)
summary(fit.nox)
fit.rm <- lm(crim ~ rm)
summary(fit.rm)
fit.dis <- lm(crim ~ dis)
summary(fit.dis)
fit.age <- lm(crim ~ age)
summary(fit.age)
fit.rad <- lm(crim ~ rad)
summary(fit.rad)
fit.tax <- lm(crim ~ tax)
summary(fit.tax)
fit.ptratio <- lm(crim ~ ptratio)
summary(fit.ptratio)
fit.black <- lm(crim ~ black)
summary(fit.black)
fit.lstat <- lm(crim ~ lstat)
summary(fit.lstat)
fit.medv <- lm(crim ~ medv)
summary(fit.medv)
```
From the summary of each predictor, we see small p-value less than 0.05 except "chaos". We can conclude there is a statistically significant association between the all predictors except "chaos" and the response.

(b)
```{r}
fit.all <- lm(crim ~ ., data = Boston)
summary(fit.all)
```
 We can reject the null hypotheses of "zn", "dis", "rad", "black" and "medv", since they all less than 0.05.
 
(c)
```{r}
simple.reg <- vector("numeric",0)
simple.reg <- c(simple.reg, fit.zn$coefficient[2])
simple.reg <- c(simple.reg, fit.indus$coefficient[2])
simple.reg <- c(simple.reg, fit.chas$coefficient[2])
simple.reg <- c(simple.reg, fit.nox$coefficient[2])
simple.reg <- c(simple.reg, fit.rm$coefficient[2])
simple.reg <- c(simple.reg, fit.age$coefficient[2])
simple.reg <- c(simple.reg, fit.dis$coefficient[2])
simple.reg <- c(simple.reg, fit.rad$coefficient[2])
simple.reg <- c(simple.reg, fit.tax$coefficient[2])
simple.reg <- c(simple.reg, fit.ptratio$coefficient[2])
simple.reg <- c(simple.reg, fit.black$coefficient[2])
simple.reg <- c(simple.reg, fit.lstat$coefficient[2])
simple.reg <- c(simple.reg, fit.medv$coefficient[2])
mult.reg <- vector("numeric", 0)
mult.reg <- c(mult.reg, fit.all$coefficients)
mult.reg <- mult.reg[-1]
plot(simple.reg, mult.reg, col = "red")

cor(Boston[-c(1, 50)])
```

The simple and multiple regression coefficients can be quite different.This difference stems from the fact that in the simple regression case, the slope term represents the average effect of a unit increase in  crime rate, ignoring other predictors. In contrast, in the multiple regression setting, the coefficient for each predictor represents the average effect of increasing crime rate while holding other predictor fixed. It still makes sense for the multiple regression to suggest no relationship between one predictor and crime rate while the simple linear regression implies the opposite.Consider the correlation matrix, the relationship between zn and rm is 0.31199059. This reveals a tendency that the area with high zn also has high rm. Now suppose that the multiple regression is correct and rm has no direct impact on crime rate, but zn has direct impact on crime rate. Then the crime rate of towns with high zn will tend to be higher, and as our correlation matrix shows, rm will also be higher in those areas. 


 
(d)
```{r}
fit.zn2 <- lm(crim ~ poly(zn, 3))
summary(fit.zn2)
fit.indus2 <- lm(crim ~ poly(indus, 3))
summary(fit.indus2)
fit.nox2 <- lm(crim ~ poly(nox, 3))
summary(fit.nox2)
fit.rm2 <- lm(crim ~ poly(rm, 3))
summary(fit.rm2)
fit.age2 <- lm(crim ~ poly(age, 3))
summary(fit.age2)
fit.dis2 <- lm(crim ~ poly(dis, 3))
summary(fit.dis2)
fit.rad2 <- lm(crim ~ poly(rad, 3))
summary(fit.rad2)
fit.tax2 <- lm(crim ~ poly(tax, 3))
summary(fit.tax2)
fit.ptratio2 <- lm(crim ~ poly(ptratio, 3))
summary(fit.ptratio2)
fit.black2 <- lm(crim ~ poly(black, 3))
summary(fit.black2)
fit.lstat2 <- lm(crim ~ poly(lstat, 3))
summary(fit.lstat2)
fit.medv2 <- lm(crim ~ poly(medv, 3))
summary(fit.medv2)
```
P-values suggest that the cubic coefficient is not statistically significant for “zn”, “rm”, “rad”, “tax” and “lstat”. However, the p-values suggest the cubic coefficient is statistically significant for “indus”, “nox”, “age”, “dis”, “ptratio” and “medv” predictors. The p-value of "black" suggests that neither the quandratic and cubic coefficients are  statistically significant.